<h2 id="overview">Overview</h2>

<p>As the name of this website states, this is just my pastedump. By no means is this an all-round best-practice guide. If your use case is to have a light touch on an ansible-master in EC2 wihtout the need to implement ‘ansible-pull’, then I’d consider this an OK guide.</p>

<p>This is what the setup will look like:</p>

<ul>
  <li>1 control instance in EC2 called ansible-master (public subnet)</li>
  <li>Multiple target instances in EC2 (private subnet/s)</li>
  <li>Target instances will belong to different stacks/roles. This will be controlled via EC2 tags</li>
  <li>No credentials wil be stored on the master instance. We’ll use SSH SendEnv and AgentForward capabilities.</li>
</ul>

<h2 id="aws">AWS</h2>

<h3 id="iam">IAM</h3>

<p>Create an “ansible-master” user which has permissions to Describe* on RDS, EC2 and ElastiCache. Download the credentials. You will use them later in <strong>~/.bashrc</strong></p>

<pre>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:Describe*",
                "elasticache:Describe*",
                "rds:Describe*"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}
</pre>

<h3 id="ssh-keys">SSH Keys</h3>

<p>Create or import “ansible-master” and “ansible-controller” SSH keys in the EC2 console.</p>

<ul>
  <li>ansible-controller = Workstation -&gt; Ansible master instance</li>
  <li>ansible-target = Workstation -(ForwardAgent)-&gt; Ansible master -&gt; Ansible targets</li>
</ul>

<p>This setup allows us to keep all credentials (EC2/SSH) on our local workstation, and just forward those credentials to the “ansible-master” instance when we SSH into it. This suits only my specific use case where I have very little automation on the Ansible side.</p>

<h3 id="launch-target-instances">Launch target instances</h3>

<p>We will test them later, for now, launch them:</p>

<ul>
  <li>Into private subnets</li>
  <li>SSH key: ansible-target</li>
  <li>Tag: AnsibleSlave=True</li>
  <li>Tag: Role=Web</li>
  <li>Tag: Stack=Prod</li>
</ul>

<p>The Role/Stack are optional, but useful if you want to play around with how you can limit execution based on EC2 tags. If you decide to go with these optional tags, spin up multiple instances and change up the Role and Stack tags among them.</p>

<p>You never need to know any other EC2 instance details except for the tags. Ansible will figure out the private IPs and other info based on said tags.</p>

<h2 id="workstation">Workstation</h2>

<h3 id="sshconfig">~/.ssh/config</h3>

<pre>Host ansible
    HostName xx.xx.xx.xx
    User ec2-user
    SendEnv AWS_ACCESS_KEY_ID
    SendEnv AWS_SECRET_ACCESS_KEY 
    IdentityFile ~/.ssh/ansible-controller
    ForwardAgent yes 
</pre>

<h3 id="bashrc">~/.bashrc</h3>

<pre>alias ansconn='AWS_ACCESS_KEY_ID="SECRET" AWS_SECRET_ACCESS_KEY="ACCESS" ssh ansible'</pre>

<h2 id="ansible-master-instance">Ansible master instance</h2>

<h3 id="ssh-daemon">SSH Daemon</h3>

<p>We want to be able to accept the environment variables from the SSH client from our workstation. In order to make this happen, we need to change the sshd_config</p>

<p>/etc/ssh/sshd_config</p>

<pre>AcceptEnv AWS_ACCESS_KEY_ID
AcceptEnv AWS_SECRET_ACCESS_KEY
</pre>

<h3 id="ansible-installation">Ansible installation</h3>

<pre>sudo pip install ansible
sudo pip install ansible --upgrade
</pre>

<p>The default ansible hosts file is plain-text and consists of your hosts and host groups. In EC2, it makes more sense to use the EC2 inventory hosts script which replaces the hosts file. This hosts file also requires the ec2.ini file on which we’ll touch on in a short bit.</p>

<pre>sudo wget 'https://raw.github.com/ansible/ansible/devel/contrib/inventory/ec2.py' -O /etc/ansible/hosts
chmod +x /etc/ansible/hosts
sudo wget 'https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.ini' -O /etc/ansible/ec2.ini
</pre>

<h3 id="ec2ini">ec2.ini</h3>

<p>It’s time to change ec2.ini settings.
Note: Adjust this on your own accord. The file is nicely commented. I’ll note some of the settings I adjusted</p>

<pre>[ec2]
regions = eu-central-1
regions_exclude = us-gov-west-1,cn-north-1
destination_variable = private_dns_name
vpc_destination_variable = private_ip_address
rds = False
elasticache = False
cache_max_age = 0
instance_filters = tag:AnsibleSlave=True
</pre>

<p>The last option might be the most significant one. Only instances with this tag will be considered as Ansible-controllable</p>

<h2 id="time-to-test">Time to test</h2>

<ul>
  <li>SSH into the instance by running <strong>ansconn</strong>, which is based on the alias we created in <strong>~/.bashrc</strong></li>
  <li>
    <p>Run:</p>

    <pre>ansible all -m ping</pre>
  </li>
  <li>
    <p>Run:</p>

    <pre>ansible tag_Role_Web -m ping</pre>
  </li>
</ul>

<p>If you implement playbooks, you can fine-grain your prod/stage stacks with a layered approach, but this is exaplined in Ansible docs.</p>
